# 5. Large and Fast: Exploiting Memory Hierarchy
## Principle of Locality
- Programs은 어느 한 시점에 전체 주소 공간 중 일부만 접근한다
- 시간 지역성(Temporal locality)
  - 최근에 접근한 항목은 가까운 시점에 다시 접근할 가능성이 높다
  - 예: 반복문 내의 명령어(instructions in a loop), 반복적으로 사용하는 변수(유도 변수,  induction variables)
- 공간 지역성(Spatial locality)
  - 최근에 접근한 항목 주변에 있는 항목도 곧 접근할 가능성이 높다
  - 예: 순차적인 명령어 접근(sequential instruction access), 배열 데이터(array data)

## Taking Advantage of Locality
- 메모리 계층 구조(Memory hierarchy)
  - 모든 데이터를 디스크(disk)에 저장한다. disk: 느리지만 큰 memory
  - 최근에 접근한 항목(및 그 근처의 항목)을 디스크에서 더 작은 DRAM 메모리로 복사한다. DRAM: 적당히 빠르지만, 약간 작은 memory
    - Main memory = DRAM
  - 더 최근에 접근한 항목(및 그 주변)을 DRAM에서 더 작은 SRAM 메모리로 복사한다. SRAM: 더 작지만 훨씬 빠른 memory
    - 이 SRAM은 CPU에 연결된 캐시 메모리(Cache memory)
   
## Memory Hierarchy Levels
- 블록(Block, 또는 라인 Line): 복사의 단위 (block, line단위로 한꺼번에 가져온다.)
  - 하나의 블록은 여러 워드(word)로 구성될 수 있음(64B, 32B)
- 접근하려는 데이터가 상위 계층(upper level)에 존재할 경우
  - 히트(Hit): 상위 계층에서 접근이 처리됨
    - 히트 비율(Hit ratio): hits(히트 수) / accesses(전체 접근 수)
- 접근하려는 데이터가 존재하지 않을 경우(absent)
  - 미스(Miss): 블록이 하위 계층(lower lever)에서 복사되어 옴
    - 걸리는 시간(Time taken): 미스 패널티(Miss penalty) -> 더 멀리서 가져오면 miss penalty 증가.
    - Miss ratio: misses/accesses <br>
                = 1 - hit ratio
  - 그런 다음, 접근된 데이터는 상위 계층(upper level)으로부터 제공된다. -> 한번 miss를 upper level에서 처리 <br>
  ![image](https://github.com/user-attachments/assets/ce1a04f1-3d60-4e76-aa24-2992daa97c68)

 ## Memory Technology
- Static RAM (SRAM) - cache memory 구성
  - 0.5ns ~ 5ns(1~10 cycle), $400 ~ $800 per GB -> 비싸서 조금만 구성
- Dynamic RAM (DRAM) - main memoey 구성 -> 프로그램에 사용하는 주소 main memory에, 돌아가는 process data는 main memory에
  - 20ns ~ 50ns(40~100 cycle), $2 ~ $4 per GB
- NAND Flash (SSD)
  - 250us ~ 1000us, $0.04 ~ $0.10 per GB
- Magnetic disk(HDD)
  - 2ms ~ 20ms, $0.01 ~ $0.02 per GB
- SSD와 HDD는 storage이다.
- 이상적인 메모리(Ideal memory)
  - SRAM의 접근 속도
  - 디스크의 용량과 GB당 비용
    
 ##  Cache Memory
- 캐시 메모리(Cache memory)
  - CPU에 가장 가까운 메모리 계층 -> 빠르게 접근, 주소로 접근
- 접근한 데이터 X₁, ..., Xₙ₋₁, Xₙ이 주어졌을 때:
  - 해당 데이터가 캐시에 있는지 어떻게 알 수 있을까? -> cache에는 일부 데이터만 있어서
  - 어디에서(어떤 위치에서) 찾아야 할까? <br>

![image](https://github.com/user-attachments/assets/94a0f518-1cce-4384-ba1c-dc1862be2012)

## Direct Mapped Cache
- 위치는 주소(address)에 의해 결정된다
- 직접 매핑 방식(Direct mapped): 선택지는 오직 하나
  - (Block address) modulo (#Blocks in cache)
    - #Blocks은 2의 거듭제곱
    - low-order address bits를 사용한다. <br>

![image](https://github.com/user-attachments/assets/9a6cc1ab-9151-46d3-ba47-e8f9196eb0e6) <br>
8개 block이여서 $2^3$이므로 3개의 하위bit를 사용한다. 그리고 mod 8한 결과에 따라 cache에 넣는다.

## Tags and Valid Bits
- 특정 캐시 위치에 어떤 블록이 저장되어 있는지 어떻게 알 수 있을까?
  - 데이터뿐 아니라 블록 주소도 함께 저장한다
  - 실제로는 주소의 상위 비트만 저장하면 된다
  - Called the **tag**
- 만약 어떤 위치에 데이터가 없으면 어떻게 할까?
  - 유효 비트(valid bit): 1이면 데이터가 존재함, 0이면 데이터가 없음(1 bit사용)
  - Initially 0
- 따라서 과정은 주소 하위 bits -> 위치 찾고 -> ((tag)상위 bits -> 맞는지 확인, valid bit -> 유효한지 확인)

## Cache Example
- 8-blocks, 1 word/block, direct mapped
- Initial state <br>
  ![image](https://github.com/user-attachments/assets/451b9b3f-915a-47a0-91ef-a793e63a5e7c)
- data 접근<br>
  ![image](https://github.com/user-attachments/assets/29df8a8d-abfa-4a0c-9366-71383260d34e)<br>
  결과: <br>
  ![image](https://github.com/user-attachments/assets/f4db652d-3cec-4a45-8dc3-2c05f75f90dd)

- data접근(2)<br>
  ![image](https://github.com/user-attachments/assets/0eb03d2c-8e6d-4dfc-8e6e-1790502d7746)<br>
  결과: <br>
  ![image](https://github.com/user-attachments/assets/4a9ff25f-7d62-4824-a8eb-1f9e017ffccc)

- data접근(3)<br>
  ![image](https://github.com/user-attachments/assets/9c205a0e-ab9d-4d6a-9590-98ee8027c14d)

- data접근(4)<br>
  ![image](https://github.com/user-attachments/assets/8d369a88-3381-4de1-9232-f396a5a96251)

- data접근(5)<br>
  ![image](https://github.com/user-attachments/assets/498cfedb-418b-4850-b985-e1cf995e015c)

## Address Subdivision
![image](https://github.com/user-attachments/assets/1bc47132-774f-48b3-b69c-d1aa1e96c36e)

## Example: Larger Block Size
![image](https://github.com/user-attachments/assets/116a6a48-df9e-461f-af0f-806e2ebb4e6c)

## Block Size Considerations
- Larger blocks은 miss rate 감소시킨다.
  - 공간 지역성(spatial locality) 때문
- 고정 크기 캐시(fixed-sized cache)에서는 문제 발생
  - Larger blocks -> fewer of them(block 크기 감소)
    - 더 많은 경쟁 -> miss rate 증가 -> 인접하지 않은 것끼리 경쟁
  - Larger blocks -> pollution(가져왔지만 쓰이지 않는 공간)
- Larger miss penalty -> miss일때 해당 data를 main memory로 부터 가져오는데 걸리는 시간
  - miss rate 감소가 이점을 상쇄 할 수 있다.
  - 조기 재시작(early restart)과 중요 워드 우선(critical-word-first)이 도움이 될 수 있음

## Cache Misses
- 캐시 히트(cache hit) 시에는 CPU가 정상적으로 진행함 -> 1 cycle내에 가능
- 캐시 미스(cache miss) 시에는 -> data가져오는데 걸리는 시간 증가.
  - CPU 파이프라인이 멈춤(stall 발생) -> pipeline 막는다. 바로 아래의 경우까지
  - Fetch block from next level(main memory) of hierarchy
  - Instruction cache miss (I-cache)-> locality증가
    - Restart instruction fetch(IF)
  - 데이터 캐시 미스(data cache miss)일 경우 (D-cache)
    - 데이터 접근을 완료함(Mem)

## Write-Through (Store -> mem write 하면 불일치 발생)
- On data-write hit, 캐시의 블록만 업데이트할 수 있다.
  - 캐시와 메모리의 데이터가 불일치 -> 해결 방법 2가지 write through와 write-back 
- write through: 메모리도 함께 업데이트
- But makes writes 시간이 더 걸린다.
  - ex) if base CPI = 1, 10% of instructions are stores, write to memory takes 100 cycles
    - Effective CPI = 1 + 0.1×100 = 11 -> 증가한다. 성능문제 발생
- 해결책: write buffer
  - 메모리에 기록될 데이터를 임시로 저장 (buffer에만 적용)
  - CPU는 바로 다음 작업을 계속 진행
    - 쓰기 버퍼(write buffer)가 가득 찬 경우에만 CPU가 대기(stall)함

## Write-Back
- Alternative: On data-write hit, 캐시 블록만 업데이트
  - 각 블록이 dirty 여부를 추적 -> 각 cache block마다 dirty block을 하나씩 둔다. <br>
  ![image](https://github.com/user-attachments/assets/66aca53e-e69c-47bb-9299-a28eaf792883) <br>
- When a dirty block is replaced -> 교체 될때만 memory에 반영
  - memory에 다시 기록(write back)
  - Can use a write buffer to allow replacing block to be read first
- 장점: memory까지 갈일없다. 교체될때만 

## Write Allocation
- What should happen on a write miss?
- Alternatives for write-through (아래 2가지중 선택가능)
  - Allocate on miss: fetch the block
  - Write around: don’t fetch the block
    - 프로그램이 종종 블록 전체를 읽기 전에 먼저 쓰기 때문(예: 초기화)
- For write-back (write allocation과 쓰인다. 보통 이조합이 많이 쓰인다)
  - Usually fetch the block<br>

![image](https://github.com/user-attachments/assets/864799cb-4372-4bc7-a750-68e19c0ff117)

## Example: Intrinsity FastMATH
- Embedded MIPS processor
  - 12-stage pipeline
  - Instruction and data access on each cycle
- Split cache: separate I-cache and D-cache
  - Each 16KB: 256 blocks × 16 words/block => 16KB => 14B / $2^8 blocks * 2^6 B/block$이고, $2^6 = 2^4 * 2^2$
  - D-cache: write-through or write-back
- SPEC2000 miss rate
  - I-cache: 0.4% -> 낮음(locality가 높아서), 매 instruction마다 접근
  - D-cache: 11.4% -> load/store inst만 접근
  - Weighted average: 3.2%

## Example: Intrinsity FastMATH(중요X)
![image](https://github.com/user-attachments/assets/c00112b9-5e85-415e-a3a7-58dcbae0c87c)

## Measuring Cache Performance
- CPU time 구성요소
  - Program execution cycles
    - Includes cache hit time
  - Memory stall cycles -> miss마다 cycle이 증가한다.
    - Mainly from cache misses
- With simplifying assumptions <br>
![image](https://github.com/user-attachments/assets/07227cb3-d005-41a7-9501-cb45957b70cb)

## Cache Performance Example
- Given
  - I-cache miss rate = 2%
  - D-cache miss rate = 4%
  - Miss penalty = 100 cycles
  - Base CPI (ideal cache) = 2
  - Load & stores are 36% of instructions
- Miss cycles per instruction
  - I-cache: 0.02 × 100 = 2
  - D-cache: 0.36 × 0.04 × 100 = 1.44
- Actual CPI = 2 + 2 + 1.44 = 5.44
  - Ideal CPU is 5.44/2 = 2.72 times faster -> cache때문에 느려질 수 있다.
 
## Average Access Time
- Hit time도 performance에 중요하다.
- Average memory access time (AMAT)
  - AMAT = Hit time + Miss rate × Miss penalty
- Example
  - CPU with 1ns clock, hit time = 1 cycle, miss penalty = 20 cycles, I-cache miss rate = 5%
  - AMAT = 1 + 0.05 × 20 = 2ns(instruction 한정)
 
## Performance Summary
- CPU 성능이 향상되면
  - 미스 패널티(캐시 미스 시 발생하는 지연)가 더 중요해짐. -> memory 갔다오는 속도가 CPU에 비해 빨라지지 않았다.
- base CPI 감소 할 수록
  - 전체 실행 시간 중 메모리 대기(memory stall) 비중이 더 커짐.
- clock rate 증가할수록
  - Memory stalls account(차지한다) for more CPU cycles 
- 따라서 시스템 성능 평가 시 캐시 동작을 반드시 고려해야 함
- cache miss로 인한 성능하락 감소를 위해
  - miss rate 감소
  - miss penalty 감소
 
## Associative Caches (miss rate 감소 방법)
 • Fully associative
 • Allow a given block to go in any cache entry
 • Requires all entries to be searched at once
 • Comparator per entry (expensive)
 • n-way set associative
 • Each set contains n entries
 • Block number determines which set
 • (Block number) modulo (# Sets in cache)
 • Search all entries in a given set at once
 • ncomparators (less expensive)

- Fully associative -> 모든 주소가 아무 block이나 쓸 수 있다.
  - 특정 블록이 캐시의 어떤 엔트리에도 들어갈 수 있음
  - 모든 엔트리를 한 번에 검색해야 하므로, 각 엔트리마다 비교기가 필요(비용이 많이 듦)
- n-way set associative(위의 절충안으로 후보지가 n개)
  - 각 집합(set)이 n개의 엔트리를 가짐
  - 선택된 집합 내 모든 엔트리를 한 번에 검색
  - n개의 비교기만 필요(비용이 더 적음)<br>

![image](https://github.com/user-attachments/assets/a05c35c3-79f7-406e-8a18-5709cd93c3a3)

## Associative Cache Example
![image](https://github.com/user-attachments/assets/7fec02dc-6bbd-4526-89b6-2ef5b1b73895)

## Spectrum of Associativity
- 8개 entries가 있는 cache의 경우 <br>
![image](https://github.com/user-attachments/assets/30cdf72d-45fb-43d2-b4b7-a525edc98675)

## Associativity Example
 • Compare 4-block cache
 • Direct mapped, 2-way set associative, fully associative
 • Block access sequence: 0, 8, 0, 6, 8 -> 이 순으로 접근(Block 주소)
 • Direct mapped
- 4-block caches를 비교한다.
  - Direct mapped, 2-way set associative, fully associative
  - Block 접근 순서: 0, 8, 0, 6, 8
- Direct mapped <br>

![image](https://github.com/user-attachments/assets/d0cd2d8d-5b68-4268-9a6c-3ee6979db5b8)
- 2-way set associative(2-way여서 하위 1bit만 한다) <br>

![image](https://github.com/user-attachments/assets/c859c56e-680f-4cf3-8a90-d792525181e8)
- Fully associative <br>

![image](https://github.com/user-attachments/assets/18857d74-deca-4edd-aaa6-f45bfc27f3d3) <br>
내려갈수록 점점 좋아진다.

## How Much Associativity
- associativity(연관도)가 증가하면 miss rate(미스율)이 감소한다
  - 하지만 그 효과는 점점 줄어든다 -> linaer하게 줄진 않는다.
- 64KB 데이터 캐시(D-cache), 16-word blocks, SPEC2000을 사용한 system Simulation
  - 1-way: 10.3%
  - 2-way: 8.6%
  - 4-way: 8.3%
  - 8-way: 8.1%
  - 줄어들긴 하지만 way가 늘어난 만큼 줄진 않는다.

## Set Associative Cache Organization
![image](https://github.com/user-attachments/assets/9f09c359-a98f-450a-ba66-9e3821831708)

## Replacement Policy -> set일때 어떤거랑 겨체 할지
- Direct mapped: 선택권이 없다
- Set associative -> way 개수 만큼 선택지
  - non-valid entry가 있다면 그것을 선호한다.
  - 그렇지 않으면 집합 내 항목들 중에서 선택한다.
- Least-recently used (LRU)
  - 가장 오랫동안 사용되지 않은 것(unused)을 선택해 교체한다.
    - 2-way에선 simple, 4-way에선 manageable, 그 이상에선 너무 어렵다. <br>
    -> 문제점: 누가 언제 마지막인지 기록 추가 필요(or 순서라도)
- Random
  - high 연관도(associativity)에서는 LRU와 거의 같은 performance를 낸다 -> 생각보다 성능 good

## Multilevel Caches -> 중간에 계층을 1개 더 만들어 너무 멀리 가지는 않게 하자
- CPU에 연결된 Primary cache(기본 캐시) (=L-1 cache)
  - 칸이 적지만, 빠르다. -> hit time을 줄이는 용도
- Level-2 cache는 primary cache에서의 misses를 처리한다
  - primary cache보다 더 크고 느리지만 여전히 main memory보다는 빠르다 -> 하나를 더 둔다.
- main memory는 L2 cache misses를 처리한다
- 일부 high-end systems에는 L-3 cache가 포함된다
- 처음에 큰거를 두지 않는 이유: 클수록 느리다(물리적 거리 증가 및 찾는 시간 증가) <br>
  -> primary cache는 hit일때 빨리 찾는 용도로 만들었다. -> L-2, L-3는 miss penalty를 줄이는 용도

## Multilevel Cache Example
- Given
  - CPU base CPI(항상 cache hit인 이상적인 CPU) = 1, clock rate = 4GHz -> clock cycle time = 0.25ns
  - Miss rate/instruction(명령어당 미스율) = 2% -> 100번중 2번 cache miss
  - Main memory access time = 100ns
- primary cache만 있는 경우
  - Miss penalty = 100ns/0.25ns = 400 사이클 -> miss 1번시 걸리는 시간
  - Effective CPI = 1 + 0.02 × 400 = 9
- L-2 cache를 추가
  - Access time = 5ns (L-1 보다 20배 느림)
  - Global miss rate to main memory = 0.5% -> 한 instruction당 평균적으로 몇번의 main memory 접근을 필요로 하는지
- Primary miss with L-2 hit
  - Penalty = 5ns/0.25ns = 20 cycles
  - 아까의 2%가 L-2로, 그리고 전체의 0.5%가 main-memory로 접근 -> L2 miss rate 25% = 0.5%/2%
- Primary miss with L-2 miss
  - Extra penalty = 400 cycles
- CPI = 1 + 0.02 × 20 + 0.005 × 400 = 3.4
- Performance ratio = 9/3.4 = 2.6 -> 성능이 2.6배 좋아졌다. <br>

![image](https://github.com/user-attachments/assets/a9f0ad8b-3dad-4c35-a665-5147e9cf4139)

## Multilevel Cache Considerations
- Primary cache
  - hit time minimal에 집중 -> 그래서 크기를 작게 만든다.
- L-2 cache
  - main memory access을 피하기 위해 low miss rate에 집중 -> 크기는 커도 되고, miss rate를 낮추는게 목표
  - Hit time은 전체 성능에 덜 영향을 미친다. 
- Results
 • L-1 cache는 보통 single cache보다 작다. -> 빠르게 하기 위해

## Interactions with Advanced CPUs
- Out-of-order(비순차 실행) CPU는 cache miss 동안에도 instructions을 실행할 수 있다
  - 보류 중(Pending)인 store는 load/store unit에 남아 있다
  - Dependent instructions은 reservation stations에서 대기한다
    - Independent instructions는 계속 실행된다
- miss의 영향은 program data flow에 따라 달라진다
  - 분석이 훨씬 더 어렵다
  - 시스템 시뮬레이션을 사용한다
  - 즉, 평가하기 어렵다

## Interactions with Software
- Misses는 memory access patterns에 따라 달라진다
  - 알고리즘의 동작 방식
  - memory access에 대한 Compiler optimization(최적화) <br>

![image](https://github.com/user-attachments/assets/bd194e59-9747-47e0-a64c-e0bc0c304a4c)

## DGEMM Access Pattern
![image](https://github.com/user-attachments/assets/710c5d40-8ee1-498b-90d8-7f2cd5d3f783)

## Cache Blocked  - block 단위로 matrix 곱을 진행한다.
![image](https://github.com/user-attachments/assets/ac2fe1b4-b750-45e3-ac04-5b0d8abec22f)

## Blocked DGEMM Access Pattern
![image](https://github.com/user-attachments/assets/f09e4cdb-bbd9-45bd-8ad0-b107989610ec)

## Dependability(신뢰성)
- Fault: 구성 요소의 고장(HW + SW)
  - 시스템 고장으로 이어질 수도 있고 아닐 수도 있다
 
## Dependability Measures
- Reliability: fault가 발생할때 까지의 평균 시간 (MTTF)
- Service interruption: 평균 수리 시간 (MTTR)
- 평균 고장 간격
  - MTBF = MTTF + MTTR
- Availability = MTTF / (MTTF + MTTR)
- Improving Availability
  - MTTF 증가: fault 회피, fault 허용, fault 예측 -> failure 잘 안일어나게
  - MTTR 감소: 진단 및 수리를 위한 향상된 도구와 절차 -> repair시간 줄이기
 
## The Hamming SEC Code
- SEC: single error correction -> 1bit의 error는 즉각수리가능
  • Hamming distance
 • Number of bits that are different between two bit patterns
 • Minimum distance = 2 provides single bit error detection
 • e.g., parity code
 • Minimum distance = 3 provides single error correction, 2 bit error detection

- Hamming distance
  - 두 비트 패턴 간에 다른 비트의 수
- Minimum distance = 2-bit는 single error detection 가능
  - 예: parity code
- Minimum distance = 3bit는 single error correction,, 2 bit error detection 가능 <br>

![image](https://github.com/user-attachments/assets/f641fb96-bee5-4b68-a40a-198355a82cdb)

## Encoding SEC <여기서부터 시작>
- To calculate Hamming code:
  - Number bits from 1 on the left
  - All bit positions that are a power 2 are parity bits
  - Each parity bit checks certain data bits:
  - 8-bit 데이터를 4-bit parity bit 추가해서 저장 -> cost 1.5배 된다<br>

![image](https://github.com/user-attachments/assets/4c75f5c3-2e7f-4d72-9451-b6de05c67a9c)

## Decoding SEC
- Value of parity bits indicates which bits are in error
  - use numbering from encoding procedure
  - e.g.,
    - Parity bits = 0000 indicates no error
    - Parity bits = 1010 indicates bit 10 was flipped <br>

![image](https://github.com/user-attachments/assets/1e9a4e7b-8232-4ceb-a9e8-12b7ec1b5a0f)


