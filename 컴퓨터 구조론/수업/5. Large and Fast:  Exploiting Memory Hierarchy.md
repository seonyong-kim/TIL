# 5. Large and Fast: Exploiting Memory Hierarchy
## Principle of Locality
- Programs은 어느 한 시점에 전체 주소 공간 중 일부만 접근한다
- 시간 지역성(Temporal locality)
  - 최근에 접근한 항목은 가까운 시점에 다시 접근할 가능성이 높다
  - 예: 반복문 내의 명령어(instructions in a loop), 반복적으로 사용하는 변수(유도 변수,  induction variables)
- 공간 지역성(Spatial locality)
  - 최근에 접근한 항목 주변에 있는 항목도 곧 접근할 가능성이 높다
  - 예: 순차적인 명령어 접근(sequential instruction access), 배열 데이터(array data)

## Taking Advantage of Locality
- 메모리 계층 구조(Memory hierarchy)
  - 모든 데이터를 디스크(disk)에 저장한다. disk: 느리지만 큰 memory
  - 최근에 접근한 항목(및 그 근처의 항목)을 디스크에서 더 작은 DRAM 메모리로 복사한다. DRAM: 적당히 빠르지만, 약간 작은 memory
    - Main memory = DRAM
  - 더 최근에 접근한 항목(및 그 주변)을 DRAM에서 더 작은 SRAM 메모리로 복사한다. SRAM: 더 작지만 훨씬 빠른 memory
    - 이 SRAM은 CPU에 연결된 캐시 메모리(Cache memory)
   
## Memory Hierarchy Levels
- 블록(Block, 또는 라인 Line): 복사의 단위 (block, line단위로 한꺼번에 가져온다.)
  - 하나의 블록은 여러 워드(word)로 구성될 수 있음(64B, 32B)
- 접근하려는 데이터가 상위 계층(upper level)에 존재할 경우
  - 히트(Hit): 상위 계층에서 접근이 처리됨
    - 히트 비율(Hit ratio): hits(히트 수) / accesses(전체 접근 수)
- 접근하려는 데이터가 존재하지 않을 경우(absent)
  - 미스(Miss): 블록이 하위 계층(lower lever)에서 복사되어 옴
    - 걸리는 시간(Time taken): 미스 패널티(Miss penalty) -> 더 멀리서 가져오면 miss penalty 증가.
    - Miss ratio: misses/accesses <br>
                = 1 - hit ratio
  - 그런 다음, 접근된 데이터는 상위 계층(upper level)으로부터 제공된다. -> 한번 miss를 upper level에서 처리 <br>
  ![image](https://github.com/user-attachments/assets/ce1a04f1-3d60-4e76-aa24-2992daa97c68)

 ## Memory Technology
- Static RAM (SRAM) - cache memory 구성
  - 0.5ns ~ 5ns(1~10 cycle), $400 ~ $800 per GB -> 비싸서 조금만 구성
- Dynamic RAM (DRAM) - main memoey 구성 -> 프로그램에 사용하는 주소 main memory에, 돌아가는 process data는 main memory에
  - 20ns ~ 50ns(40~100 cycle), $2 ~ $4 per GB
- NAND Flash (SSD)
  - 250us ~ 1000us, $0.04 ~ $0.10 per GB
- Magnetic disk(HDD)
  - 2ms ~ 20ms, $0.01 ~ $0.02 per GB
- SSD와 HDD는 storage이다.
- 이상적인 메모리(Ideal memory)
  - SRAM의 접근 속도
  - 디스크의 용량과 GB당 비용
    
 ##  Cache Memory
- 캐시 메모리(Cache memory)
  - CPU에 가장 가까운 메모리 계층 -> 빠르게 접근, 주소로 접근
- 접근한 데이터 X₁, ..., Xₙ₋₁, Xₙ이 주어졌을 때:
  - 해당 데이터가 캐시에 있는지 어떻게 알 수 있을까? -> cache에는 일부 데이터만 있어서
  - 어디에서(어떤 위치에서) 찾아야 할까? <br>

![image](https://github.com/user-attachments/assets/94a0f518-1cce-4384-ba1c-dc1862be2012)

## Direct Mapped Cache
- 위치는 주소(address)에 의해 결정된다
- 직접 매핑 방식(Direct mapped): 선택지는 오직 하나
  - (Block address) modulo (#Blocks in cache)
    - #Blocks은 2의 거듭제곱
    - low-order address bits를 사용한다. <br>

![image](https://github.com/user-attachments/assets/9a6cc1ab-9151-46d3-ba47-e8f9196eb0e6) <br>
8개 block이여서 $2^3$이므로 3개의 하위bit를 사용한다. 그리고 mod 8한 결과에 따라 cache에 넣는다.

## Tags and Valid Bits
- 특정 캐시 위치에 어떤 블록이 저장되어 있는지 어떻게 알 수 있을까?
  - 데이터뿐 아니라 블록 주소도 함께 저장한다
  - 실제로는 주소의 상위 비트만 저장하면 된다
  - Called the **tag**
- 만약 어떤 위치에 데이터가 없으면 어떻게 할까?
  - 유효 비트(valid bit): 1이면 데이터가 존재함, 0이면 데이터가 없음(1 bit사용)
  - Initially 0
- 따라서 과정은 주소 하위 bits -> 위치 찾고 -> ((tag)상위 bits -> 맞는지 확인, valid bit -> 유효한지 확인)

## Cache Example
- 8-blocks, 1 word/block, direct mapped
- Initial state <br>
  ![image](https://github.com/user-attachments/assets/451b9b3f-915a-47a0-91ef-a793e63a5e7c)
- data 접근<br>
  ![image](https://github.com/user-attachments/assets/29df8a8d-abfa-4a0c-9366-71383260d34e)<br>
  결과: <br>
  ![image](https://github.com/user-attachments/assets/f4db652d-3cec-4a45-8dc3-2c05f75f90dd)

- data접근(2)<br>
  ![image](https://github.com/user-attachments/assets/0eb03d2c-8e6d-4dfc-8e6e-1790502d7746)<br>
  결과: <br>
  ![image](https://github.com/user-attachments/assets/4a9ff25f-7d62-4824-a8eb-1f9e017ffccc)

- data접근(3)<br>
  ![image](https://github.com/user-attachments/assets/9c205a0e-ab9d-4d6a-9590-98ee8027c14d)

- data접근(4)<br>
  ![image](https://github.com/user-attachments/assets/8d369a88-3381-4de1-9232-f396a5a96251)

- data접근(5)<br>
  ![image](https://github.com/user-attachments/assets/498cfedb-418b-4850-b985-e1cf995e015c)

## Address Subdivision
![image](https://github.com/user-attachments/assets/1bc47132-774f-48b3-b69c-d1aa1e96c36e)

## Example: Larger Block Size
![image](https://github.com/user-attachments/assets/116a6a48-df9e-461f-af0f-806e2ebb4e6c)

## Block Size Considerations
- Larger blocks은 miss rate 감소시킨다.
  - 공간 지역성(spatial locality) 때문
- 고정 크기 캐시(fixed-sized cache)에서는 문제 발생
  - Larger blocks -> fewer of them(block 크기 감소)
    - 더 많은 경쟁 -> miss rate 증가 -> 인접하지 않은 것끼리 경쟁
  - Larger blocks -> pollution(가져왔지만 쓰이지 않는 공간)
- Larger miss penalty -> miss일때 해당 data를 main memory로 부터 가져오는데 걸리는 시간
  - miss rate 감소가 이점을 상쇄 할 수 있다.
  - 조기 재시작(early restart)과 중요 워드 우선(critical-word-first)이 도움이 될 수 있음

## Cache Misses
- 캐시 히트(cache hit) 시에는 CPU가 정상적으로 진행함 -> 1 cycle내에 가능
- 캐시 미스(cache miss) 시에는 -> data가져오는데 걸리는 시간 증가.
  - CPU 파이프라인이 멈춤(stall 발생) -> pipeline 막는다. 바로 아래의 경우까지
  - Fetch block from next level(main memory) of hierarchy
  - Instruction cache miss (I-cache)-> locality증가
    - Restart instruction fetch(IF)
  - 데이터 캐시 미스(data cache miss)일 경우 (D-cache)
    - 데이터 접근을 완료함(Mem)

## Write-Through (Store -> mem write 하면 불일치 발생)
- On data-write hit, 캐시의 블록만 업데이트할 수 있다.
  - 캐시와 메모리의 데이터가 불일치 -> 해결 방법 2가지 write through와 write-back 
- write through: 메모리도 함께 업데이트
- But makes writes 시간이 더 걸린다.
  - ex) if base CPI = 1, 10% of instructions are stores, write to memory takes 100 cycles
    - Effective CPI = 1 + 0.1×100 = 11 -> 증가한다. 성능문제 발생
- 해결책: write buffer
  - 메모리에 기록될 데이터를 임시로 저장 (buffer에만 적용)
  - CPU는 바로 다음 작업을 계속 진행
    - 쓰기 버퍼(write buffer)가 가득 찬 경우에만 CPU가 대기(stall)함

## Write-Back
- Alternative: On data-write hit, 캐시 블록만 업데이트
  - 각 블록이 dirty 여부를 추적 -> 각 cache block마다 dirty block을 하나씩 둔다. <br>
  ![image](https://github.com/user-attachments/assets/66aca53e-e69c-47bb-9299-a28eaf792883) <br>
- When a dirty block is replaced -> 교체 될때만 memory에 반영
  - memory에 다시 기록(write back)
  - Can use a write buffer to allow replacing block to be read first
- 장점: memory까지 갈일없다. 교체될때만 

## Write Allocation
- What should happen on a write miss?
- Alternatives for write-through (아래 2가지중 선택가능)
  - Allocate on miss: fetch the block
  - Write around: don’t fetch the block
    - 프로그램이 종종 블록 전체를 읽기 전에 먼저 쓰기 때문(예: 초기화)
- For write-back (write allocation과 쓰인다. 보통 이조합이 많이 쓰인다)
  - Usually fetch the block<br>

![image](https://github.com/user-attachments/assets/864799cb-4372-4bc7-a750-68e19c0ff117)

## Example: Intrinsity FastMATH
- Embedded MIPS processor
  - 12-stage pipeline
  - Instruction and data access on each cycle
- Split cache: separate I-cache and D-cache
  - Each 16KB: 256 blocks × 16 words/block => 16KB => 14B / $2^8 blocks * 2^6 B/block$이고, $2^6 = 2^4 * 2^2$
  - D-cache: write-through or write-back
- SPEC2000 miss rate
  - I-cache: 0.4% -> 낮음(locality가 높아서), 매 instruction마다 접근
  - D-cache: 11.4% -> load/store inst만 접근
  - Weighted average: 3.2%

## Example: Intrinsity FastMATH(중요X)
![image](https://github.com/user-attachments/assets/c00112b9-5e85-415e-a3a7-58dcbae0c87c)

## Measuring Cache Performance
- CPU time 구성요소
  - Program execution cycles
    - Includes cache hit time
  - Memory stall cycles -> miss마다 cycle이 증가한다.
    - Mainly from cache misses
- With simplifying assumptions <br>
![image](https://github.com/user-attachments/assets/07227cb3-d005-41a7-9501-cb45957b70cb)

## Cache Performance Example
- Given
  - I-cache miss rate = 2%
  - D-cache miss rate = 4%
  - Miss penalty = 100 cycles
  - Base CPI (ideal cache) = 2
  - Load & stores are 36% of instructions
- Miss cycles per instruction
  - I-cache: 0.02 × 100 = 2
  - D-cache: 0.36 × 0.04 × 100 = 1.44
- Actual CPI = 2 + 2 + 1.44 = 5.44
  - Ideal CPU is 5.44/2 = 2.72 times faster -> cache때문에 느려질 수 있다.
 
## Average Access Time
- Hit time도 performance에 중요하다.
- Average memory access time (AMAT)
  - AMAT = Hit time + Miss rate × Miss penalty
- Example
  - CPU with 1ns clock, hit time = 1 cycle, miss penalty = 20 cycles, I-cache miss rate = 5%
  - AMAT = 1 + 0.05 × 20 = 2ns(instruction 한정)
 
## Performance Summary
- CPU 성능이 향상되면
  - 미스 패널티(캐시 미스 시 발생하는 지연)가 더 중요해짐. -> memory 갔다오는 속도가 CPU에 비해 빨라지지 않았다.
- base CPI 감소 할 수록
  - 전체 실행 시간 중 메모리 대기(memory stall) 비중이 더 커짐.
- clock rate 증가할수록
  - Memory stalls account(차지한다) for more CPU cycles 
- 따라서 시스템 성능 평가 시 캐시 동작을 반드시 고려해야 함
- cache miss로 인한 성능하락 감소를 위해
  - miss rate 감소
  - miss penalty 감소
 
## Associative Caches (miss rate 감소 방법)
 • Fully associative
 • Allow a given block to go in any cache entry
 • Requires all entries to be searched at once
 • Comparator per entry (expensive)
 • n-way set associative
 • Each set contains n entries
 • Block number determines which set
 • (Block number) modulo (# Sets in cache)
 • Search all entries in a given set at once
 • ncomparators (less expensive)

- Fully associative -> 모든 주소가 아무 block이나 쓸 수 있다.
  - 특정 블록이 캐시의 어떤 엔트리에도 들어갈 수 있음
  - 모든 엔트리를 한 번에 검색해야 하므로, 각 엔트리마다 비교기가 필요(비용이 많이 듦)
- n-way set associative(위의 절충안으로 후보지가 n개)
  - 각 집합(set)이 n개의 엔트리를 가짐
  - 선택된 집합 내 모든 엔트리를 한 번에 검색
  - n개의 비교기만 필요(비용이 더 적음)<br>

![image](https://github.com/user-attachments/assets/a05c35c3-79f7-406e-8a18-5709cd93c3a3)
