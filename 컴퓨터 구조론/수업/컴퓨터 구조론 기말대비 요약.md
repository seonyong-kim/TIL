# 4. The Processor
## ALU Control
- Load/Store -> add
- Branch -> subtract
- T-type -> funct field에 따라 결정<br>

## The Main Control Unit
- opcode에서 파생된 2비트를 ALUOp로 한다
- Combinational logic은 ALU 제어를 파생<br>
![image](https://github.com/user-attachments/assets/7a624270-e584-40c1-a1a6-959be95753ef)
<br> R타입의 경우 뒤에 추가적인 decoding을 한다.

## The Main Control Unit
- Control signals은 instruction에서 파생 <br>
![image](https://github.com/user-attachments/assets/cf55f87f-a345-4a97-8325-22bbd103ca06)

## Datapath with Control
![image](https://github.com/user-attachments/assets/445883ae-e24c-46d9-a49c-af8a5b427997) <br>
|종류|R-type|lw|sw|branch|
|---|---|---|---|---|
|RegDst|1|0|X|X|
|ALUSrc|0|1|1|0|
|MemtoReg|0|1|0|X|
|RegWrite|1|1|0|0|
|MemRead|0|1|0|0|
|MemWrite|0|0|1|0|
|Branch|0|0|0|1|
|ALUOp|10|00|??|01| <br>
- RegDst: 결과를 저장할 register(rd면 1 / rt면 0)
- ALUSRC: 0이면 register 사용, 1이면 즉시값
- pcsrc: beq && ALU 결과 조건 만족시: 1 / 나머지는 0

## Implementing Jumps
![image](https://github.com/user-attachments/assets/d780aff1-fa68-4224-82d0-015679f9b109) <br>
4-bit(이전 PC 상위 4bit) 26bit(jump address값) 2bit(00 고정)해서 32 bit
- Jump는 word address
- opcode에서 추가 control signal decoded가 필요하다

## Datapath with Jumps Added
![image](https://github.com/user-attachments/assets/cbacedf4-dda3-4e7f-a5af-5c38d38ec643)<br>

## Performance Issues
- 가장 긴 delay가 clock period를 결정
  - critical path는 load instruction
  - 과정은 instruction memory -> register file → ALU → data memory → register file
- 설계 원칙 위반
  - 일반적인 사례를 빠르게 처리 -> Load에 맞추면 비효율적이다.
- 파이프라이닝을 통해 성능을 향상시킬 수 있다.

## MIPS Pipeline
- Five stages, one step per stage
  - IF: memory에서 instruction 가져오기
  - ID: Instruction 해석 & register 읽기
  - EX: 산술연산 or 주소계산(ALU)
  - MEM: memory에 읽기/쓰기
  - WB: register에 결과물 작성
 
## Pipeline Performance
- 제일긴게 Cycle로 설정 -> 손해보는게 생긴다.
5각단계마다 걸리는 시간이 달라서 pipelined인지에 따라 달라진다.<br>
![image](https://github.com/user-attachments/assets/b6637734-0cb8-4f99-8b34-2ea313448eb8)

## Pipeline Speedup
- 모든 단계가 균형 잡혀 있다면 (즉, 각 단계가 같은 시간 소요) <br>
  ![image](https://github.com/user-attachments/assets/1cb0e023-df2e-4fff-bba6-639ac170ada4)
- 균형이 맞지 않으면 속도 향상은 줄어듦
- 속도 향상은 처리량 증가 덕분
  - Latency(각 명령어가 끝날 때까지 걸리는 시간)은 줄어들지 않는다.

## Pipelining and ISA Design
- MIPS ISA는 파이프라이닝을 고려해서 설계됨
  - 모든 instructions는 32-bits: one cycle에 가져오고 해석하기 쉬움
  - instruction formats이 적고 규칙적: one step에 명령어 해석과 레지스터 읽기를 처리 가능
  - Load/store addressing: 3단계에서 주소 계산, 4단계에서 메모리 접근
  - memory operands 정렬: 메모리 접근이 한 사이클이면 충분함(memory에 1번만 접근 가능)
 
## Hazards
다음 cycle에 다음 instruction를 시작하지 못하게 만드는 상황들
- Structure hazards(구조적 위험): resource 충돌
- Data hazard(데이터 위험): 이전 instruction의 data 읽기/쓰기를 끝낼 때까지 기다려야 한다
- Control hazard(제어 위험): branch경우 ALU를 해야 다음에 어떻게 할지 나온다.

## Structural Hazards
파이프라인에서 두 명령어가 같은 자원을 사용할 때 발생하는 문제
- MIPS 파이프라인에서 하나의 메모리만 사용하는 경우:
  - load/store 명령어는 데이터 접근이 필요함
  - 동시에 instruction을 가져올 수 없어서 대기해야 함 → 이로 인해 파이프라인에 bubble이 생김
  - 그래서 파이프라인 구조에서는 instruction/data memories를 분리 or instruction/data caches를 따로 사용
 
## Data Hazards
한 instruction이 이전 instruction의 데이터 접근이 완료되기를 기다려야 하는 상황이다. <be>
데이터가 준비 안되어 있는데 register dependency가 있을때 <br>

![image](https://github.com/user-attachments/assets/a6b10ba8-4338-4046-bb9a-1f06ea877000)

## Forwarding (aka Bypassing)
datapath에 추가 연결을 통해 결과가 계산되면 즉시 사용 <br>
![image](https://github.com/user-attachments/assets/d4123fe1-2c41-4bf4-a224-70e34881040b)

## Load-Use Data Hazard
필요한 시점에 값이 아직 계산되지 않았다면 시간을 거슬러 forward 할 수 없다. <br>
![image](https://github.com/user-attachments/assets/764bf9f2-c816-4251-8c69-f87298da5208)

## Code Scheduling to Avoid Stalls
서로 상관없는 instruction이 다음에 오도록 하려고 코드 순서를 재정렬할 수 있다. <br>
![image](https://github.com/user-attachments/assets/83127892-1f8b-4c40-81c4-0fe832d54226)<br>

## Control Hazards
branch를 할지 말지 알 수 없어서 bubble 발생
- Branch 여부는 ID이후에 나온다. 그래서 바로 IF가 있으면 위험
- In MIPS pipeline
  - registers 비교와 target address 계산을 pipeline 초기에 해야 한다.
  - 이를 위해 ID 단계에서 이 작업을 할 수 있도록 하드웨어를 추가해야 한다.

## Stall on Branch
- branch 결과가 결정될 때까지 기다린 후, 다음 instruction를 가져온다. <br>
![image](https://github.com/user-attachments/assets/2838b4de-6fb3-45da-82f8-882530a5b809)

## Branch Prediction
- 긴 pipelines은 branch 결과를 일찍 결정하기 어려움
- branch 결과를 예측하고, 예측이 틀린 경우에만 Stall을 발생시킨다
- MIPS는 branch가 안일어 난다고 가정하고 다음 instruction을 실행한다.

## More-Realistic Branch Prediction
- Static branch prediction: 일반적인 branch 동작에 기반
  - loop and if-statement branche의 경우
  - 뒤로 가는 branch(루프)는 taken(jump한다)으로 예측
  - 앞으로 가는 branch(if)는 not taken(다음 줄로 이동)으로 예측
- Dynamic branch prediction: Hardware가 실제 브랜치 결과를 측정
  - 과거 경향(history)을 바탕으로 미래 동작을 예측 -> 틀리면 re-fetching하고, update

## Pipeline Summary
- Pipelining은 instruction throughput(처리율)을 높여 성능을 향상시킨다
- Subject to hazards(위험요소들)
  - Structure, data, control
- Instruction 집합 구조(ISA)가 pipeline 구현의 복잡성에 영향을 준다.
  
## Pipeline Registers
- registers사이에 stages가 필요. -> clock이 바뀔때만(rising edge) 동작하는 register
  - 이전 cycle에서 생산한 정보를 가지고 있어야 한다. <br>
![image](https://github.com/user-attachments/assets/4234d8b9-9a86-4e0a-a3b2-49f089760430)

## Pipeline Operation
- pipelined datapath를 통한 instruction의 사이클별 흐름
  - "Single-clock-cycle" pipeline diagram
    - 하나의 cycle에서 pipeline사용 보여준다.
  - "multi-clock-cycle" diagram과 비교하면
    - 시간에 따른 동작의 그래프

## IF for Load, Store, …
![image](https://github.com/user-attachments/assets/be9349d3-3afa-4f40-97c2-f67827594ab4)

## ID for Load, Store, …
![image](https://github.com/user-attachments/assets/056ddcd4-472e-4fa0-8521-9cf6cfa542af)

## EX for Load
![image](https://github.com/user-attachments/assets/d0370330-750b-4881-9074-18041acdd8ba)

## MEM for Load
![image](https://github.com/user-attachments/assets/cac5d52f-c59c-46a9-ad0a-2aa9a678b237)

## WB for Load
![image](https://github.com/user-attachments/assets/7a6a8451-5b75-4092-9fa3-13b3e9f40740)

## Corrected Datapath for Load(write register도 같이 전달 되도록 수정)
![image](https://github.com/user-attachments/assets/9a4971fb-4ca5-4ca6-8c73-dff2f55d6ec7)

## EX for Store
![image](https://github.com/user-attachments/assets/6b71eb03-bfee-456a-bbcf-59016826189a)

## MEM for Store
![image](https://github.com/user-attachments/assets/99dcb359-d48b-4149-96e3-2782ce39c140)

## WB for Store
![image](https://github.com/user-attachments/assets/4d833011-3245-42b3-9366-821ed3673662)
## Multi-Cycle Pipeline Diagram
- resource 사용 형식<br>
![image](https://github.com/user-attachments/assets/3220569e-6a7a-4512-ac5f-0732e68aa1be)<br>

![image](https://github.com/user-attachments/assets/739d73bd-db75-4134-a5df-2005caf91446)

## Pipelined Control (Simplified)
![image](https://github.com/user-attachments/assets/45b25f55-339a-4b60-850b-07034d8f6008)

## Pipelined Control
- Control signals은 instruction에서 발생(ID에서) <br>
![image](https://github.com/user-attachments/assets/852953ea-5847-4cd6-8403-825e93725cd1)

## Pipelined Control
![image](https://github.com/user-attachments/assets/3bcf9b18-0557-471f-b396-944121621335)

## Data Hazards in ALU Instructions(R-format)
- Consider this sequence: <br>
```
sub $2, $1,$3
and $12,$2,$5
or  $13,$6,$2
add $14,$2,$2
sw $15,100($2)
```
reigister $2가 계속 사용
- forwarding으로 위험을 해결가능 -> 언제 감지?

## Dependencies & Forwarding
![image](https://github.com/user-attachments/assets/09e022b4-98e2-4e64-b198-4725c33c1d20)

## Detecting the Need to Forward
- pipeline을 따라 register 번호를 전달한다.
- ALU operand register numbers in EX stage에서의 ALU 피연산자 register 번호는
  - ID/EX.RegisterRs, ID/EX.RegisterRt로 주어진다.
- Data hazards는 다음과 같은 상황에 발생
  - EX/MEM.RegisterRd = ID/EX.RegisterRs / ID/EX.RegisterRt
  - MEM/WB.RegisterRd = ID/EX.RegisterRs / ID/EX.RegisterRt<br>
- register번호는 Decoding때 알 수 있다.

## Detecting the Need to Forward
- forwarding instruction이 register에 값을 쓸때만 해당. 즉, 필요할때 만
- 그리고 해당 명령어의 Rd가 $zero 아닐 때만 -> $zero면 덮어쓰는게 불가능 
 
## Forwarding Paths
![image](https://github.com/user-attachments/assets/d81421ff-f8ba-497f-952e-549321d72fa5) <br>
앞에서 받은 Rs,Rt와 뒤에서 결과로 받은 것을 비교해서 forwarding 필요여부 판단 <br>
-> 아후 forwarding A와 forwarding B 신호에 따라 mux에 어떤 값을 받을지 결정

## Double Data Hazard
```
add $1,$1,$2
add $1,$1,$3
add $1,$1,$4 // EX/MEM 단계의 값을 forwarding 판단 + MEM/WB 단계의 값을 forwarding 판단 
```
$1의 경우 둘다 forwarding이 필요. 하지만 1번만 가능. 따라서 <br>
1. register 번호 체크
2. EX/MEM forwarding이 없을 경우에만 
- 두 가지 hazard가 모두 발생할 수 있다.
  - 이 경우 가장 최신의 값(EX/MEM)을 사용해야 한다.
- 따라서 MEM 단계의 hazard 조건을 수정해야 한다.
  - 단, EX 단계의 hazard 조건이 참이 아닐 때만 MEM 단계에서 forwarding을 수행하도록 해야 한다.

## Revised Forwarding Condition
- MEM hazard
![image](https://github.com/user-attachments/assets/79dbe9fc-5502-42e4-b30e-3688a1e69fca) <br>
![image](https://github.com/user-attachments/assets/b02a3eaa-ab5a-445a-a273-5a77d5a9a5ef)

## Datapath with Forwarding(흐름만 이해)
![image](https://github.com/user-attachments/assets/83e7b3e3-d301-4d07-b99d-6b9efd967f67)

## Load-Use Data Hazard
![image](https://github.com/user-attachments/assets/fb9f1bfe-a43b-47a9-92db-4a19ccd3b561)

## Load-Use Hazard Detection
- instruction을 사용하는 시점은 ID단계에서 decoded된다.
- Load-use hazard은 다음 조건일 때 발생한다:
  - ID/EX.MemRead and (lw인지 확인) <br>
      ((ID/EX.RegisterRt = IF/ID.RegisterRs) or <br>
        (ID/EX.RegisterRt = IF/ID.RegisterRt)) <br>
    ![image](https://github.com/user-attachments/assets/4db62006-f3ea-43c1-a1cb-7e81109f8bfe)
- 이런 위험이 감지되면, 파이프라인을 정지(stall)시키고 bubble을 삽입한다.(모든 control signal에)

## How to Stall the Pipeline
- ID/EX 레지스터에 있는 제어 신호(control values)를 0으로 강제로 설정한다
  - EX, MEM, WB 단계는 nop (no-operation), 즉 아무 작업도 하지 않도록 만든다
- PC(프로그램 카운터)와 IF/ID 레지스터의 갱신을 막는다
  - 로드된 값을 사용하는 명령어는 다시 디코딩되고
  - 그 다음 명령어는 다시 fetch 된다
  - 1 사이클 동안 stall(정지)하면, MEM 단계에서 lw 명령어가 데이터를 메모리에서 읽을 수 있으며
    - 이후 그 값을 EX 단계에 전달(forwarding)할 수 있게 된다 <br>

![image](https://github.com/user-attachments/assets/a7bda25c-da1f-4199-9ab9-f075ecd292c0)

## Load-Use Data Hazard
![image](https://github.com/user-attachments/assets/b125209e-fe9b-4ccd-9399-4476c97867ad) 

## Datapath with Hazard Detection
bubble이면 control이 0이 된다. -> mux에 0값이 들어오면 bublle을 만든다

## Stalls and Performance
- Stall은 1clock을 날려서 성능을 저하 -> 정확한 결과를 얻기 위해서는 필요
- 컴파일러는 코드 재배치를 통해 위험와 stall을 피할 수 있다

## Branch Hazards
- 브랜치(분기) 결과가 MEM 단계에서 결정된다면 <br>

![image](https://github.com/user-attachments/assets/6945c67f-9992-4d67-a85e-44c509d69451)

## Reducing Branch Delay
- 결과를 판단하는 HW를 ID 단계로 이동시킨다. (HW 최적화) => 결정을 빠르게하자. (다음을 ID에서)
  - Target address adder
  - Register comparator

## Example: Branch Taken
![image](https://github.com/user-attachments/assets/f4bc9439-ff53-42fa-a4fe-cd2ecd9bec3e) <br>

## Data Hazards for Branches
- 비교에 사용되는 레지스터가 앞서 실행된 두 번째 또는 세 번째 목적지인 경우 <-> forwarding br>
![image](https://github.com/user-attachments/assets/a3330f99-f0b3-4f2e-a341-ecf4f532042e)<br>
- 비교에 사용되는 레지스터가 바로 앞의 목적지이거나, 두 번째 앞선 load 명령어의 목적지인 경우
  - 1 사이클의 stall이 필요하다 왜?<br>
  -> beq는 ID에서 register 비교를 진행한다. <br>
  ![image](https://github.com/user-attachments/assets/1d34c071-4d00-40a0-86b8-1a84102683ec)
- 비교에 사용되는 레지스터가 바로 직전에 수행된 load 명령어의 목적지 레지스터인 경우
  - 2 사이클의 정지(stall)가 필요하다. <br>
![image](https://github.com/user-attachments/assets/98a6d80c-1923-4bbe-9204-7e28bf14b45c)

## Dynamic Branch Prediction 
- 분기 패널티(branch penalty)가 더 중요해진다.
- 동적 예측(dynamic prediction)을 사용한다.
  - Branch prediction buffer (aka branch history table)를 사용 -> 이전 분기 결과(taken/not taken) 저장
  - 최근의 분기 명령어 주소를 인덱스로 사용한다.
  - branch 명령어 실행할 때
    - 테이블을 확인하고 이전과 같은 결과를 예측한다.
    - fall-through 경로나 분기 대상지에서 명령어 가져오기를 시작한다.
    - 예측이 틀리면 파이프라인을 비우고(flush) 예측을 반대로 바꾼다.<br>
    즉, bubble로 하고 table에 T->N/ N->T로 한다. <br>
  
![image](https://github.com/user-attachments/assets/4be09458-b4ab-4c3c-8561-fc1f6bd89e61)

## 1-Bit Predictor: Shortcoming
- Inner loop branches 두번 mispredicted <br>
![image](https://github.com/user-attachments/assets/b3432096-b7f8-4b79-bf44-53f94c6e83d0) <br>
  - 내부 loop 마지막 반복 + 다음번 loop 시작 -> 오예측
  - 해결방법: history를 늘린다.(전 + 전전기록) -> 2-Bit
 
## 2-Bit Predictor
- 오직 두 번 오예측되었을 때만 예측을 변경한다. <br>
![image](https://github.com/user-attachments/assets/ba54d11c-732b-44de-a9ae-8730a13f930f)

## 2-Bit Predictor (another ver)
- 두 번 연속으로 오예측이 발생했을 때에만 예측을 변경한다. <br>
![image](https://github.com/user-attachments/assets/e993d099-8749-4893-8331-0940c9420736)

## Calculating the Branch Target 
- 예측기를 사용해도 분기 대상 주소를 계산 -> branch가 실행될 경우 1사이클의 패널티 발생(bubble)
- Branch target buffer
  - 대상 주소를 저장하는 캐시
  - 명령어가 가져올 때 PC로 인덱싱
    - 적중(hit)하고 분기가 실행될 것으로 예측되면, 즉시 대상 주소를 가져올 수 있다 <br>
    만약 적중해도 칸수 부족으로 없으면 바로 불가능<br>
![image](https://github.com/user-attachments/assets/b71353ba-fe24-4bfc-b8b6-c54f13ce017d)<br>
![image](https://github.com/user-attachments/assets/0f6b0d02-b1fc-4a3b-8caa-36fa89c394ca)

## Instruction-Level Parallelism (ILP)
- Pipelining: 여러 명령어를 병렬로 실행
- ILP(Instruction-Level Parallelism)를 높이기 위한 방법 -> system처리량을 높인다.
  - 더 깊은 파이프라인(pipeline길게 즉, 길하나 길게)
    - 각 단계에서 수행하는 일을 줄이면 → 클럭 사이클이 짧아진다. <br>
    ![image](https://github.com/user-attachments/assets/89cfd603-b094-4067-9a68-36ec01e88551)<br>
  - Multiple issue(길 여러개 -> 여러 instruction을 동시에 발행)
    - 파이프라인 단계를 복제하여 → 여러 개의 파이프라인 운영 -> 클럭 사이클당 여러 명령어를 시작한다.
    - CPI < 1이 되므로 IPC(Instructions Per Cycle, 한싸이클에 instruction 몇개?) 지표를 사용
    - 그러나 실제로는 종속성 때문에 IPC는 줄어든다. -> 달성하기 힘들다.(but, 성능 향상에는 기여)<br>

![image](https://github.com/user-attachments/assets/2660e05d-cffe-46a6-86c2-11c3658762e8)

## Multiple Issue
- Static multiple issue -> PC,PC+4를 항상 같이 fetch, SW가 hazard나 dependency 해결
  - Compiler가 함께 발행될 명령어들을 그룹으로 묶고, issue slots이라는 묶음으로 패키징
  - Compiler가 hazards를 감지하고 회피한다. -> 감지하고 알아서 피해준다.
- Dynamic multiple issue -> HW(CPU)가 책임, CPU가 instruction을 여러개 미리 fetch -> 그중 상관없는 것들을 모아서 같이 issue
  - CPU가 명령어 스트림을 실행 중에 분석하고, 매 사이클마다 발행할 명령어를 선택
  - Compiler는 명령어 재배치 등을 통해 CPU를 도울 수 있다
  - CPU는 실행 중에 고급 기술로 hazard를 해결한다

## Speculation
- 명령어에 대해 무엇을 해야 할지 추측하는거
  - 가능한 한 빨리 연산을 시작한다.
  - 추측이 맞았는지 확인
    - 맞으면 연산을 완료
    - 틀리면 롤백하고 올바른 작업 수행
- Common to static and dynamic multiple issue
- Examples
  - branch outcome 추측
    - 실제로 따라야 할 경로가 다르면 롤백
  - load 추측
    - 해당 위치가 나중에 수정되면 롤백 <br>
   
![image](https://github.com/user-attachments/assets/3e3132e9-e812-464c-9bfd-c76d7b7c68a8)

## Compiler/Hardware Speculation(복구의 책임)
- 컴파일러는 명령어를 재배치할 수 있다(static)
  - 잘못된 추측에서 복구하기 위한 수정(fix-up) 명령어를 포함할 수 있다. (compiler가 recover하는 code추가)
- 하드웨어는 실행할 명령어를 미리 살펴볼 수 있다(dynamic)
  - 실제로 필요하다는 것이 확인될 때까지 결과를 버퍼에 저장함 -> 실행되도되면 그때 적용
  - 잘못된 추측이었을 경우 버퍼를 비운다.

## Speculation and Exceptions
- 추측 실행된 명령어에서 예외가 발생하면 어떻게 될까?
- Static speculation(추측 실행) -> ISA(명령어 집합 구조)에 예외를 지연시키는 기능을 추가할 수 있다
- Dynamic speculation(추측 실행) -> 예외를 버퍼에 저장해두고, 해당 명령어가 완전히 완료될 때까지 예외 처리를 미룬다 (명령어가 끝까지 수행되지 않을 수도 있다)
 
## Static Multiple Issue(한번에 2개 issue를 grouping한다)
- Compiler는 명령어들을 issue packets으로 묶는다.
  - 한 사이클에 실행할 수 있는 명령어들의 그룹
  - 파이프라인 자원 요구 사항에 따라 결정됨
- issue packet을 a very long instruction처럼 생각할 수 있다.
  - 여러 개의 동시 실행 작업을 지정함
  - → Very Long Instruction Word (VLIW)

## Scheduling Static Multiple Issue
- Compiler는 일부 또는 모든 하드웨어적인 hazard를 제거해야 한다.
  - 명령어들을 issue packet으로 재정렬한다.
  - 하나의 패킷 안에는 dependency이 없어야 한다.
  - 패킷 간에는 종속성이 존재할 수 있다.
    - 이는 ISA(명령어 집합 구조)마다 다르며, 컴파일러가 이를 알고 있어야 한다!
  - 필요하다면 nop(아무 작업도 하지 않는 명령어)으로 패딩한다.

## MIPS with Static Dual Issue
- Two-issue packet (두 개 명령어로 구성된 이슈 패킷)
  - 하나는 ALU 또는 분기(branch) 명령어  /  다른 하나는 load 또는 store 명령어
  - 위 둘은 역활을 다르게 길을 나눈다. 
  - 64비트 정렬됨 -> 64bit는 2개씩 묶여있어서 8씩 건너뛴다. -> PC = PC + 8
    - 순서는 ALU/branch -> load/store 명령어 
    - 사용되지 않는 명령어 자리는 nop으로 채운다 <br>
![image](https://github.com/user-attachments/assets/edc71909-8263-4bdb-baba-189e2603f4bf)

## Hazards in the Dual-Issue MIPS
- 더 많은 명령어가 병렬로 실행됨
- EX 단계에서의 data hazard
  - 같은 패킷 안에서 ALU 결과를 load/store 명령어에서 사용할 수 없다
    ```
    add  $t0, $s0, $s1
    load $s2, 0($t0)
    ```
    forwarding때문에 하나의 packet 불가능 
    - 두 개의 패킷으로 나누어야 하므로, 결과적으로 stall이 발생함
- Load-use hazard
  - 여전히 1사이클의 사용 지연(latency)이 존재하지만, 이제는 두 개의 명령어가 있음 -> 근데 nop으로 채워 보낸다면 2개 손해가 난다.
- 더 공격적인(적극적인) 스케줄링이 필요함 -> reorder 필요

## Scheduling Example
- Schedule this for dual-issue MIPS <br>
![image](https://github.com/user-attachments/assets/1c9a8ccc-afaf-49d5-8abd-42bafd766fbf) <br>
-> addu $s1, $s1, -4가 sw $t0, 4($s1)보다 위에 있어도 되는 이유는 pointer이고, 4($s1)이므로 결과적으로는 같다.

## Loop Unrolling
- parallelism을 더 많이 드러내기 위해 loop body를 복제한다 -> 이거는 static multiple issue쓸때만 유용
  - loop-control에 드는 오버헤드를 줄인다(beq 비중 내리고, instruction 재배치 가능성 높인다.) -> dynamic, static모두 유용
- 복제된 루프마다 서로 다른 레지스터를 사용한다
  - Called “register renaming”
  - 루프에서 반복마다 발생하는 반의존성(anti-dependency)을 피한다
    - 동일한 레지스터에 값을 저장한 후, 바로 그 레지스터에서 값을 읽는 경우 -> 같은 register 사용X
    - 이것을 이름 의존성(name dependence)이라고도 한다
      - 동일한 레지스터 이름을 재사용하는 것 <br>

![image](https://github.com/user-attachments/assets/8e44e975-df33-46c8-a43d-6da60931d061)

## Loop Unrolling Example
![image](https://github.com/user-attachments/assets/01593a87-cddf-482f-a273-0369dc911a9f)

## Dynamic Multiple Issue
- superscalar processors(CPU, HW가 알아서 순서를 정한다.)
- CPU가 매 사이클마다 명령어를 0개, 1개, 2개… 실행할지 결정한다
  - structural and data hazards피한다.
- compiler가 명령어 실행 순서를 정렬할 필요를 줄여준다
  - 물론 컴파일러의 스케줄링이 여전히 도움이 될 수는 있다 -> 도움은 되지만 필수는 아니다.
  - 코드의 의미(동작의 정확성)는 CPU가 보장한다
- 구체적으로 정해진 방법이 아니므로 개념적으로만 이해한다.

## Dynamic Pipeline Scheduling
- CPU가 명령어들을 순서와 다르게(out-of-order) 실행하도록 하여 stall을 피할 수 있도록 한다
  - 하지만 최종 결과를 레지스터에 기록(commit)할 때는 원래 명령어 순서대로 기록한다

## Dynamically Scheduled CPU
![image](https://github.com/user-attachments/assets/abcb028c-6758-4056-9767-e6e587e149d7)

## Register Renaming
- Reservation stations과 reorder buffer(commit unit안)는 효과적으로 register renaming 기능을 제공한다
- instruction이 reservation station에 발행(issue)될 때
  - 피연산자(operand)가 레지스터 파일이나 reorder buffer에 사용 가능한 상태로 있다면
    - reservation station으로 복사된다
    - 그러면 해당 피연산자는 더 이상 레지스터에 남아 있을 필요가 없고, 레지스터는 덮어써도 된다
  - 반대로, 피연산자가 아직 준비되지 않았다면
    - 연산 유닛(function unit)이 나중에 reservation station으로 제공할 것이다
    - 이 경우에는 레지스터를 갱신하지 않아도 될 수도 있다

## Speculation
- branch의 실제 결과가 결정되기 전까지는 commit하지 않는다
- load 추측 실행 (Load speculation)
  - 로드 지연 및 캐시 미스(cache miss) 지연을 피하기 위해 다음과 같은 추측을 수행한다: <br>
  load관련 speculation 많다.
    - 유효 주소(effective address)를 예측하고 -> $t2 + 10 미리 예측
    - 로드될 값을 예측하며 -> 이전의 읽은 값이 이번에도 쓰일까?
    - 이전에 완료되지 않은 store보다 먼저 load를 수행하고 
    - store된 값을 load 유닛에 우회 전달한다 (bypass)
  - 단, 이러한 추측이 확정되기 전까지는 load 명령어를 커밋하지 않는다

## Why Do dynamic Scheduling?
- 왜 그냥 컴파일러가 코드 스케줄링을 맡기지 않을까? -> 너무 어려워서
- 모든 stall이 예측 가능한 것은 아니기 때문이다
  - 예: cache miss는 실행 중에야 발생 여부를 알 수 있음
- branch를 중심으로 항상 스케줄링할 수 있는 것도 아니다 -> 분기 결과는 실행 중(dynamic)에 결정되기 때문이다
- 같은 ISA(명령어 집합 구조)라도 구현 방식에 따라 지연 시간(latency)이나 위험(hazard)이 다를 수 있다
- 이러한 이유때문에 dynamic으로 한다.

## Does Multiple Issue Work? (효과 있나?)
- 그렇다. 하지만 우리가 기대하는 만큼은 아니다
- 프로그램에는 실제 의존성(데이터 간 관계)이 있어서 명령어 수준 병렬성(ILP)을 제한한다
- 일부 의존성은 제거하기 어렵다
  - 예: 포인터 별칭(pointer aliasing) — 어떤 포인터가 다른 포인터와 같은 메모리를 가리킬 수 있기 때문
  - 명령어 발행 시 윈도우 크기(window size)가 제한적이라서
  - 메모리 지연(memory delay)과 대역폭 제한도 병목이 된다
  - 그래서 파이프라인을 계속 채워두기 어렵다
  - 추측 실행(speculation)은 잘만 하면 도움이 될 수 있다

## Power Efficiency
- 동적 스케줄링과 추측 실행(speculation)의 복잡성은 전력 소모를 증가시킨다
- 그래서 여러 개의 더 단순한 코어들을 사용하는 것이 더 나을 수도 있다 

## Cortex A53 and Intel i7(한번 읽어만 보자)
![image](https://github.com/user-attachments/assets/eb2d4586-f286-417b-b58c-f90adfed036d)

## ARM Cortex-A53 Branch Prediction(한번 읽어만 보자)
![image](https://github.com/user-attachments/assets/7903cebe-a4fe-4da8-98f8-9665d2e58ab9)

## ARM Cortex-A53 Performance(한번 읽어만 보자)
![image](https://github.com/user-attachments/assets/9ec50d54-bae8-49c3-a77d-df5c28668e4f)

## Another Matrix Multiply
![image](https://github.com/user-attachments/assets/14a98b12-8667-4392-a219-085e7cdcff80)

## Fallacies
- Pipelining is easy (!)
  - The basic idea is easy
  - 하지만 세부 사항이 문제다
    - 예: 데이터 위험(data hazard)을 감지하는 것 등

## Pitfalls
- 좋지 못한 ISA(명령어 집합 구조) 설계는 파이프라이닝을 더 어렵게 만들 수 있다
  - 예: 복잡한 명령어 집합(CISC) 구조인 VAX나 IA-32
    - 파이프라이닝이 작동하도록 만들기 위해 상당한 오버헤드가 필요하다
    - IA-32는 이를 위해 마이크로-옵(micro-op) 방식을 사용함

## Concluding Remarks
- ISA(명령어 집합 구조)는 데이터 경로(datapath)와 제어(control) 설계에 영향을 준다
- 반대로, 데이터 경로와 제어 설계도 ISA 설계에 영향을 준다
- 파이프라이닝은 병렬성을 활용하여 명령어 처리량(throughput)을 향상시킨다
  - 즉, 초당 더 많은 명령어를 완료할 수 있게 된다
  - 하지만 각 명령어의 지연 시간(latency)은 줄어들지 않는다
- 위험(Hazard)에는 세 가지가 있다: 구조적(structural), 데이터(data), 제어(control)
- 다중 발행(Multiple Issue)과 동적 스케줄링(Dynamic Scheduling)은 명령어 수준 병렬성(ILP: Instruction-Level Parallelism)을 활용하려는 기법이다
  - 하지만 의존성(dependency)이 병렬성 달성에 한계를 만든다
  - 그리고 그로 인해 생기는 복잡성은 전력 소모 문제(전력 벽, power wall)로 이어진다

--------------------------------------------------------------------------------------------------------------------------

# 5. Large and Fast: Exploiting Memory Hierarchy
## Principle of Locality
- Programs은 어느 한 시점에 전체 주소 공간 중 일부만 접근한다
- 시간 지역성(Temporal locality): 최근에 접근한 항목은 가까운 시점에 다시 접근할 가능성이 높다
- 공간 지역성(Spatial locality): 최근에 접근한 항목 주변에 있는 항목도 곧 접근할 가능성이 높다

## Taking Advantage of Locality
- 메모리 계층 구조(Memory hierarchy)
  - 모든 데이터를 disk(느리지만 큰 memory)에 저장
  - 최근에 접근한 항목과 근처의 항목을 disk에서 DRAM(=main memory, 적당히 빠르지만, 약간 작은 memory)으로 복사
  - 더 최근에 접근한 항목과 주변 항목을 DRAM에서 더 작은 SRAM(더 작지만 훨씬 빠른 memory, cache)으로 복사
   
## Memory Hierarchy Levels
- Block(= Line): 복사의 단위 -> block(line)단위로 한꺼번에 가져오며 한 Block은 word로 구성(64B, 32B)
- 접근하려는 data가 상위 계층(upper level)에 존재
  - Hit: 상위 계층에서 접근을 처리
    - Hit ratio: hits(히트 수) / accesses(전체 접근 수)
- 접근하려는 데이터가 존재하지 않을 경우
  - Miss: 블록이 하위 계층(lower lever)에서 복사되어 옴
    - Time taken: Miss penalty -> 더 멀리서 가져오면 miss penalty 증가.
    - Miss ratio: misses/accesses = 1 - hit ratio
  - 이후, 접근된 데이터는 상위 계층(upper level)으로부터 제공 -> 즉, 한번 miss된것은 이후에 upper level에서 처리

## Memory Technology
- Static RAM (SRAM): cache memory 구성
  - 0.5ns ~ 5ns(1~10 cycle), $400 ~ $800 per GB -> 비싸서 조금만 구성
- Dynamic RAM (DRAM): main memoey 구성 -> program 주소와 돌아가는 process data는 main memory에 있다.
  - 20ns ~ 50ns(40~100 cycle), $2 ~ $4 per GB
- NAND Flash (SSD) -> storage
  - 250us ~ 1000us, $0.04 ~ $0.10 per GB
- Magnetic disk(HDD) -> storage
  - 2ms ~ 20ms, $0.01 ~ $0.02 per GB
- 이상적인 메모리(Ideal memory)
  - SRAM의 접근 속도 + 디스크의 용량과 GB당 비용
    
##  Cache Memory
- CPU에 가장 가까운 메모리 계층 -> 빠르고, 주소로 접근
- cache에는 일부 데이터만 있어서

## Direct Mapped Cache -> 주소마다 저장될 수 있는 칸을 고정
- 위치는 address에 의해 결정된다
- Direct mapped: 선택지는 오직 하나
  - (Block address) modulo (#Blocks in cache)
    - #Blocks은 2의 거듭제곱
    - low-order address bits를 사용한다. <br>

![image](https://github.com/user-attachments/assets/9a6cc1ab-9151-46d3-ba47-e8f9196eb0e6) <br>
8개 block이여서 $2^3$이므로 3개의 하위bit를 사용한다. 그리고 mod 8한 결과에 따라 cache에 넣는다.

## Tags and Valid Bits
- tag: 주소의 상위 비트만 저장(하위 비트는 이미 저장) <br>
  -> 특정 캐시 위치에 어떤 블록이 저장되어 있는지 어떻게 알 수 있기 위해서
- valid bit(1-bit): 1이면 데이터가 존재함, 0이면 데이터가 없다. 초기값은 0
- 과정은 주소 하위 bits -> 위치 찾고 -> (tag)상위 bits -> 맞는지 확인 +  valid bit -> 유효한지 확인

## Cache Example
- 8-blocks, 1 word/block, direct mapped
- Initial state <br>
  ![image](https://github.com/user-attachments/assets/451b9b3f-915a-47a0-91ef-a793e63a5e7c)
- data 접근<br>
  ![image](https://github.com/user-attachments/assets/29df8a8d-abfa-4a0c-9366-71383260d34e)<br>
  결과: <br>
  ![image](https://github.com/user-attachments/assets/d79bc218-18da-418c-80c2-a6afca412f5a)

- data접근(2)<br>
  ![image](https://github.com/user-attachments/assets/0eb03d2c-8e6d-4dfc-8e6e-1790502d7746)<br>
  결과: <br>
  ![image](https://github.com/user-attachments/assets/2a6ce995-5c5e-4e87-a9cd-5e7cdab7dcdd)

- data접근(3)<br>
  ![image](https://github.com/user-attachments/assets/9c205a0e-ab9d-4d6a-9590-98ee8027c14d)

- data접근(4)<br>
  ![image](https://github.com/user-attachments/assets/8d369a88-3381-4de1-9232-f396a5a96251)

- data접근(5)<br>
  ![image](https://github.com/user-attachments/assets/498cfedb-418b-4850-b985-e1cf995e015c)

## Address Subdivision
![image](https://github.com/user-attachments/assets/1bc47132-774f-48b3-b69c-d1aa1e96c36e)

## Example: Larger Block Size
![image](https://github.com/user-attachments/assets/116a6a48-df9e-461f-af0f-806e2ebb4e6c)

## Block Size Considerations
- Larger blocks(큰 block)은 miss rate 감소 -> 공간적 지역성 때문
- fixed-sized cache에서는 문제 발생
  - Larger blocks -> block 개수 감소 -> 경쟁 증가 -> miss rate 증가 -> 인접하지 않은 것끼리 경쟁
  - Larger blocks -> pollution(가져왔지만 쓰이지 않는 공간)
- Larger miss penalty -> miss일때 해당 data를 main memory로 부터 가져오는데 걸리는 시간
  - miss rate 감소가 이점을 상쇄
  - early restart과 critical-word-first이 도움

## Cache Misses
- cache hit -> CPU가 정상적으로 진행 -> 1 cycle내 가능
- cache miss -> data 가져오는데 걸리는 시간 증가.
  - CPU 파이프라인이 stall -> 바로 아래의 경우까지
  - hierarchy의 다음 단계인 main memory에서 block을 불러온다(fetch)
  - Instruction cache miss (I-cache)-> locality증가
    - Restart instruction fetch(IF)
  - data cache miss(D-cache)
    - 데이터 접근을 완료함(Mem)

## Write-Through (Store -> mem write 하면 불일치 발생)
- On data-write hit, cache의 block만 업데이트
  - cache와 memory의 data가 불일치 -> 해결 방법 2가지 write through와 write-back 
- write through: 메모리도 함께 업데이트 -> makes writes 시간이 더 걸린다.
  - ex) if base CPI = 1, 10% of instructions are stores, write to memory takes 100 cycles <br>
  Effective CPI = 1 + 0.1×100 = 11 -> 증가한다. 성능문제 발생
- 해결책: write buffer
  - memory에 기록될 data를 임시로 저장 (buffer에만 적용)
  - CPU는 바로 다음 작업을 계속 진행
    - write buffer가 가득 찬 경우에만 CPU가 stall

## Write-Back
- Alternative: On data-write hit, cache block만 업데이트
  - 각 블록이 dirty 여부를 추적 -> 각 cache block마다 dirty block을 하나씩 둔다. <br>
  ![image](https://github.com/user-attachments/assets/66aca53e-e69c-47bb-9299-a28eaf792883) <br>
- cache에 있던 수정된 데이터 블록(dirty block)이 새 데이터로 인해 교체될 때 -> memory에 반영
  - memory에 write back
  - write buffer를 사용하면 교체되는 블록을 먼저 읽어오는 것이 가능
- 장점: memory까지 갈일은 교체될때만 

## Write Allocation
- write miss가 발생하면 무엇이 일어나야 하는가? <br>
  write miss: CPU가 캐시에 없는 데이터에 쓰기를 시도
- write-through의 2가지 대안
  - write allocate : block을 가져온다
  - Write around: block을 가져오지 않는다.
    - 프로그램이 종종 블록 전체를 읽기 전에 먼저 쓰기 때문(예: 초기화)
- For write-back (write allocation과 쓰인다. 보통 이조합이 많이 쓰인다)
  - 대부분 block을 가져온다. 

## Example: Intrinsity FastMATH
- Embedded MIPS processor
  - 12-stage pipeline
  - 각 cycle에 Instruction and data access
- Split cache: separate I-cache and D-cache
  - Each 16KB: 256 blocks × 16 words/block => 16KB => 14B / $2^8 blocks * 2^6 B/block$이고, $2^6 = 2^4 * 2^2$
  - D-cache: write-through or write-back
- SPEC2000 miss rate
  - I-cache: 0.4% -> 낮음(locality가 높아서), 매 instruction마다 접근
  - D-cache: 11.4% -> load/store inst만 접근
  - Weighted average: 3.2%

## Measuring Cache Performance
- CPU time 구성요소
  - Program execution cycles(cache hit time 포함)
  - Memory stall cycles(주로 cache miss때문) -> miss마다 cycle이 증가한다.
- With simplifying assumptions <br>

![image](https://github.com/user-attachments/assets/07227cb3-d005-41a7-9501-cb45957b70cb)
- 문제 예시
  - I-cache miss rate = 2%  /  D-cache miss rate = 4%
  - Miss penalty = 100 cycles
  - Base CPI (ideal cache) = 2
  - Load & stores are 36% of instructions
- Miss cycles per instruction
  - I-cache: 0.02 × 100 = 2  /  D-cache: 0.36 × 0.04 × 100 = 1.44
- Actual CPI = 2 + 2 + 1.44 = 5.44
  - Ideal CPU is 5.44/2 = 2.72 times faster -> cache때문에 느려질 수 있다.
 
## Average Access Time 
- Hit time도 performance에 중요
- Average memory access time (AMAT) = Hit time + Miss rate × Miss penalty
- Example
  - CPU with 1ns clock, hit time = 1 cycle, miss penalty = 20 cycles, I-cache miss rate = 5%
  - AMAT = 1 + 0.05 × 20 = 2ns(instruction 한정)
 
## Performance Summary
- CPU 성능이 향상되면 miss penalty가 더 중요해짐. -> memory 갔다오는 속도가 CPU에 비해 빨라지지 않았다.
- base CPI 감소 할 수록 전체 실행 시간 중 memory stall 비중이 더 커짐.
- clock rate 증가할수록 Memory stalls 차지한다 for more CPU cycles 
- 따라서 시스템 성능 평가 시 cache 동작을 반드시 고려해야 함
- cache miss로 인한 성능하락 감소를 위해
  - miss rate 감소  /  miss penalty 감소
 
## Associative Caches (miss rate 감소 방법)
- Fully associative: 특정 block cache의 어떤 entry(모든 주소)에도 들어갈 수 있다.
  - 모든 엔트리를 한 번에 검색 -> 비용이 많이 듦
- n-way set associative: 각 set이 n개의 엔트리를 가지고 선택된 집합 내 모든 entries를 한 번에 검색
  - n개의 비교기만 필요(비용이 더 적음)<br>
- Direct mapped에서의 index: block  /  N-way set associative에서의 index: set

## Associative Cache Example
![image](https://github.com/user-attachments/assets/7fec02dc-6bbd-4526-89b6-2ef5b1b73895)

## Spectrum of Associativity
- 8개 entries가 있는 cache의 경우 <br>
![image](https://github.com/user-attachments/assets/30cdf72d-45fb-43d2-b4b7-a525edc98675)

## Associativity Example
- 4-block caches를 비교
  - Block 접근 순서: 0, 8, 0, 6, 8
- Direct mapped <br>

![image](https://github.com/user-attachments/assets/d0cd2d8d-5b68-4268-9a6c-3ee6979db5b8)
- 2-way set associative(2-way여서 하위 1bit만 한다) <br>

![image](https://github.com/user-attachments/assets/c859c56e-680f-4cf3-8a90-d792525181e8)
- Fully associative <br>

![image](https://github.com/user-attachments/assets/18857d74-deca-4edd-aaa6-f45bfc27f3d3) <br>
내려갈수록 점점 좋아진다.

## How Much Associativity
- associativity(연관도)가 증가하면 miss rate이 감소
  - 하지만 그 효과는 점점 줄어든다 -> linear하게 줄진 않는다.
- 1가지 특징으로 way가 늘어나면 늘어난 만큼 줄진 않는다.

## Set Associative Cache Organization
![image](https://github.com/user-attachments/assets/9f09c359-a98f-450a-ba66-9e3821831708)

## Replacement Policy -> set일때 어떤거랑 교체 할지
- Direct mapped: 선택권이 없다
- Set associative -> way 개수 만큼 선택지
  - non-valid entry를 선호, 없으면 set 내 항목들 중에서 선택.
- Least-recently used (LRU): 가장 오랫동안 사용되지 않은 것(unused)을 선택해 교체
  - 2-way에선 simple, 4-way에선 manageable, 그 이상에선 너무 어렵다. <br>
  -> 문제점: 누가 언제 마지막인지 기록 추가 필요(or 순서라도)
- Random
  - high 연관도(associativity)에서는 LRU와 거의 같은 performance를 낸다 -> 생각보다 성능 good

## Multilevel Caches -> 중간에 계층을 1개 더 만들어 너무 멀리 가지는 않게 하자
- CPU에 연결된 Primary cache(기본 캐시) (=L-1 cache)
  - 칸이 적지만, 빠르다. -> hit time을 줄이는 용도
- Level-2 cache는 primary cache에서의 misses를 처리한다
  - primary cache보다 더 크고 느리지만 여전히 main memory보다는 빠르다 -> 하나를 더 둔다.
- main memory는 L2 cache misses를 처리한다
- 일부 high-end systems에는 L-3 cache가 포함된다
- 처음에 큰거를 두지 않는 이유: 클수록 느리다(물리적 거리 증가 및 찾는 시간 증가) <br>
  -> primary cache는 hit일때 빨리 찾는 용도로 만들었다. -> L-2, L-3는 miss penalty를 줄이는 용도

## Multilevel Cache Example
- CPU base CPI(항상 cache hit인 이상적인 CPU) = 1, clock rate = 4GHz -> clock cycle time = 0.25ns <br>
  Miss rate/instruction(명령어당 미스율) = 2% -> 100번중 2번 cache miss <br>
  Main memory access time = 100ns
- primary cache만 있는 경우 <br>
  Miss penalty = 100ns/0.25ns = 400 사이클 -> miss 1번시 걸리는 시간 <br>
  Effective CPI = 1 + 0.02 × 400 = 9
- L-2 cache를 추가
  Access time = 5ns (L-1 보다 20배 느림) <br>
  Global miss rate to main memory = 0.5% -> 한 instruction당 평균적으로 몇번의 main memory 접근을 필요로 하는지 <br>
- Primary miss with L-2 hit
  - Penalty = 5ns/0.25ns = 20 cycles
  - 아까의 2%가 L-2로, 그리고 전체의 0.5%가 main-memory로 접근 -> L2 miss rate 25% = 0.5%/2%
- Primary miss with L-2 miss
  - Extra penalty = 400 cycles
- CPI = 1 + 0.02 × 20 + 0.005 × 400 = 3.4
- Performance ratio = 9/3.4 = 2.6 -> 성능이 2.6배 좋아졌다.

## Multilevel Cache Considerations
- Primary cache -> hit time 최소화에 집중 -> 그래서 크기를 작게 만든다.
- L-2 cache -> low miss rate에 집중(main memory에 가는거 최소화) -> 크기는 커도 되고, miss rate를 낮추는게 목표
  - Hit time은 전체 성능에 덜 영향을 미친다. 
- L-1 cache는 보통 single cache보다 작다. -> 빠르게 하기 위해

## Interactions with Advanced CPUs
- Out-of-order(비순차 실행) CPU는 cache miss 동안에도 instructions을 실행
  - 보류 중(Pending)인 store는 load/store unit에 남아 있다
  - Dependent instructions은 reservation stations에서 대기 -> Independent instructions는 계속 실행
- miss의 영향은 program data flow에 따라 달라진다 -> 평가하기 어렵다

## Interactions with Software
- Misses는 memory access patterns에 따라 달라진다
  - 알고리즘의 동작 방식
  - memory access에 대한 Compiler 최적화 <br>

![image](https://github.com/user-attachments/assets/bd194e59-9747-47e0-a64c-e0bc0c304a4c)

## DGEMM Access Pattern
![image](https://github.com/user-attachments/assets/710c5d40-8ee1-498b-90d8-7f2cd5d3f783)

## Cache Blocked  - block 단위로 matrix 곱을 진행한다.
![image](https://github.com/user-attachments/assets/ac2fe1b4-b750-45e3-ac04-5b0d8abec22f)

## Blocked DGEMM Access Pattern
![image](https://github.com/user-attachments/assets/f09e4cdb-bbd9-45bd-8ad0-b107989610ec)

## Dependability(신뢰성)
- Fault: 구성 요소의 고장(HW + SW)
  - 시스템 고장으로 이어질 수도 있고 아닐 수도 있다
 
## Dependability Measures
- Reliability: fault가 발생할때 까지의 평균 시간 (MTTF)
- Service interruption: 평균 수리 시간 (MTTR)
- 평균 고장 간격(MTBF) = MTTF + MTTR
- Availability = MTTF / (MTTF + MTTR)
- Improving Availability
  - MTTF 증가: failure 잘 안일어나게
  - MTTR 감소: repair시간 줄이기
 
## The Hamming SEC Code
- SEC: single error correction -> 1bit의 error는 즉각수리가능
- Hamming distance: 주어진 두 비트 패턴 간에 다른 비트의 수
- Minimum Hamming distance = 2이면 single error detection 가능 (parity code)
- Minimum Hamming distance = 3이면 single error correction + 2 bit error detection <br>

![image](https://github.com/user-attachments/assets/f641fb96-bee5-4b68-a40a-198355a82cdb)

## Encoding SEC 
- Hamming code 계산하는 방법: -> 최소한의 parity bit으로 sec하기
  - 비트에 왼쪽부터 1번씩 번호를 매긴다.  
  - $2^n$ 꼴(즉, 1, 2, 4, 8, ...)의 bit 위치는 parity bit로 사용된다.  
  - 각 parity bit는 특정 데이터 비트들을 검사한다.
  - 8-bit 데이터를 4-bit parity bit 추가해서 저장 -> cost 1.5배 된다<br>

![image](https://github.com/user-attachments/assets/4c75f5c3-2e7f-4d72-9451-b6de05c67a9c)

## Decoding SEC
- parity bits의 Value는 error가 발생한 bits 위치를 나타낸다.  
  - encoding 과정에서 사용한 번호 매기기를 그대로 사용한다.  
  - Parity bits = 0000 → 오류 없음  /  Parity bits = 1010 → 10($p_8, p_2$)번 비트에 오류 발생 (뒤집혔다)<br>

## SEC/DED Code
- 전체 단어에 대해 추가적인 parity bit $p_n$을 추가함. -> 설명 X
- Hamming distance를 4로 만들었다 -> double error 검출도 가능
- H는 $p_1, p_2, ...$들의 합이 짝수 가 되도록 0 또는 1이 설정된다.

![image](https://github.com/user-attachments/assets/8126d700-054c-409d-abec-c1ccc006f747)<br>
![image](https://github.com/user-attachments/assets/0cafb8ae-493b-4a29-a056-2d31eec81627)

## Virtual Memory
- main memory를 secondary(disk) storage의 cache로 사용
  - CPU 하드웨어와 OS가 공동으로 관리
- 여러 Programs이 main memory를 공유  
  - 각 프로그램은 자주 사용하는 코드와 데이터를 포함한 private virtual address space를 가진다.
  - 다른 프로그램으로부터 보호된다.
- CPU와 OS는 virtual address를 physical address로 변환한다.  
  - 가상 메모리(VM)의 "block"을 page라고 부른다.  
  - translation miss는 page fault(=page miss)라고 한다. <br>

![image](https://github.com/user-attachments/assets/bf986b14-d94f-4667-a888-4dc1e7fd5480)

## Address Translation
![image](https://github.com/user-attachments/assets/11b25392-caf4-4269-a257-c463aa468262)

## Page Fault Penalty
- page fault가 발생 -> disk에서 해당 page 가져와야 -> 수백만 clock cycle이 소요 -> 들어가는 시간이 크다.
  - OS 이를 처리.
- page fault rate을 최소화 -> fully associative 방식
  - smart replacement algorithms을 사용한다.

## Page Tables - virtual page -> physical page를 직접알려주는 table
- Page table은 virtual page number를 인덱스로 해서 physical frame number(실제 물리 주소의 일부)를 저장하는 배열이다.
- page가 memory에 존재하는 경우  
  - PTE(Page Table Entry)는 physical page number를 저장한다.  
  - 추가로 참조됨 여부(referenced), 수정됨 여부(dirty) 등의 상태 비트도 포함한다. -> 컴구에서는 크게 고려X
  - valid = 1 -> memory에 있다. 0이면 memory에 없다.
- page가 메모리에 존재하지 않는 경우  
  - PTE는 디스크의 스왑 공간(swap space)에 있는 위치를 참조할 수 있다.

## Mapping Pages to Storage
![image](https://github.com/user-attachments/assets/f05abe3c-b649-4189-8ac4-e29fcad0b58e)

## Replacement and Writes -> evict 대상 선정
- page fault rate을 줄이기 위해, Least-Recently Used(LRU)를 교체하는 것이 바람직하다.  
  - 페이지에 접근하면 PTE의 Reference bit(= use bit)가 1로 설정된다. -> Reference bit는 1bit이다.
  - OS가 주기적으로 이 값을 0으로 초기화
  - reference bit가 0인 페이지는 최근에 사용되지 않았음을 의미
- disk로의 쓰기는 수백만 clock cycle이 걸린다.  
  - block 단위로 처리
  - write-through은 비현실적 -> disk에 쓰는데 오래 걸린다. 
  - write-back 방식을 사용
  - page에 쓰기가 발생하면 PTE의 dirty bit가 설정된다.

## Fast Translation Using a TLB
- Address translation은 1번의 추가 memory 접근 필요
  - PTE(Page Table Entry)를 접근 + actual memory를 접근
- page tables 접근은 locality가 좋다. -> 자주 사용되는거 모여있어서 -> TLB로 모은다
  - CPU 내부에 PTE의 빠른 cache를 사용  
  - 이를 TLB(Translation Look-aside Buffer)라고 한다. -> page table 전용 cache -> 매번 2번 접근 파하기 위해서
  - 일반적으로 TLB는 16~512개의 PTE를 저장 -> 속도가 빠르다
  - misses는 하드웨어나 소프트웨어에 의해 처리된다. <br>

![image](https://github.com/user-attachments/assets/bd3027ab-1e3c-454c-9e11-765b8afbb274)

## TLB Misses
- page가 memory에 존재하면
  - TLB hit -> 바로 memory 주소 번역 후 실제 memory 접근 (1 ~ 100cycle)
  - TLB miss -> memory가서 PTE읽는다. (100cycle) -> TLB update & 실제 memory 접근
  - HW에서 처리 -> 복잡한 page table structures에서는 어려울 수 있다.  
  - 소프트웨어에서 처리 -> 특수 예외를 발생시키고 optimized handler가 처리한다.
- page가 memory에 없으면(page fault) -> disk에서 page를 가져온다 -> page table update -> TLB update & 실제 메모리 접근
  - OS가 page를 가져오고 page table을 업데이트한다.  
  - 그런 다음 오류를 발생시킨 명령을 다시 시작한다.

## Finding a Block
![image](https://github.com/user-attachments/assets/83f284b4-660d-4be2-9e01-2c5a7c202e3c)

## Sources of Misses
- compulsory misses(= cold start misses) -> 반드시 발생 할 수 밖에 없는 miss
- capacity misses: cache size가 한정되어 있기 때문에 발생(용량 부족이 문제)  
- conflict misses(= collision misses) 
  - fully associative cache가 아닌 경우에 발생한다. -> 자리를 두고 경쟁 -> 자리차면 replacement 발생 -> <br>
  이후 접근해도 주소 사라져 있음
- capacity misses와 conflict misses 구별하는 방법 <br>
  같은 size에서 associative 늘리면 해결가능 -> conflict misses

## Cache Design Trade-offs
![image](https://github.com/user-attachments/assets/54db8d39-f56b-4f9e-9aac-35c51fa1a68b)

## Cache Coherence Problem(지금까지 cpu core 1개 가정)
- 두 개의 CPU cores가 physical address space을 공유한다고 가정한다.  
  - write-through cache를 사용한다.

![image](https://github.com/user-attachments/assets/906444e5-611e-41d0-94d2-b81cdd400965)
